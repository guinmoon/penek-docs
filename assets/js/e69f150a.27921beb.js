"use strict";(self.webpackChunk_eightshift_docs=self.webpackChunk_eightshift_docs||[]).push([[2208],{8053:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>c,contentTitle:()=>s,default:()=>p,frontMatter:()=>a,metadata:()=>i,toc:()=>l});var o=t(4848),r=t(8453);const a={},s="LoRA",i={id:"lora",title:"LoRA",description:"Put the adapter files in the lora_adapters directory.",source:"@site/docs/lora.md",sourceDirName:".",slug:"/lora",permalink:"/docs/lora",draft:!1,unlisted:!1,tags:[],version:"current",frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"Inference options",permalink:"/docs/inference_options"},next:{title:"Prompt format",permalink:"/docs/prompt_format"}},c={},l=[];function d(e){const n={a:"a",code:"code",em:"em",h1:"h1",p:"p",pre:"pre",strong:"strong",...(0,r.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(n.h1,{id:"lora",children:"LoRA"}),"\n",(0,o.jsxs)(n.p,{children:["Put the adapter files in the ",(0,o.jsx)(n.code,{children:"lora_adapters"})," directory."]}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsxs)(n.strong,{children:["You ",(0,o.jsx)(n.code,{children:"cannot use mmap"})," when connecting an adapter, so ",(0,o.jsx)(n.code,{children:"memory"})," consumption will be ",(0,o.jsx)(n.code,{children:"high"}),". In order to use mmap with lora do export."]}),"\nTo add one LoRA adapter, you can use the LLMFarm chat settings interface.\nTo add several LoRA adapters, manually write them in the configuration file like this:"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-JSON",children:'{\n...\n    "prompt_format" : "{{prompt}}",\n    "model" : "openllama-3b-v2-q8_0.gguf",\n    "lora_adapters" : [\n        {\n            "adapter" : "lora-open-llama-3b-v2-q8_0-shakespeare-LATEST.bin",\n            "scale" : 0.9\n        },\n        {\n            "adapter" : "lora-open-llama-3b-v2-q8_0-second-LATEST.bin",\n            "scale" : 1.0\n        }\n    ],\n...\n}\n'})}),"\n",(0,o.jsx)(n.h1,{id:"fine-tune",children:"Fine Tune"}),"\n",(0,o.jsxs)(n.p,{children:["Now you can perform FineTune directly in LLMFarm. To do this, go to settings -> FineTune\n",(0,o.jsx)(n.em,{children:"FineTune consumes quite a lot of RAM, so on iOS it is possible to train only 3B models with minimum settings n_ctx, n_batch"}),".\nYou can read ",(0,o.jsx)(n.a,{href:"https://github.com/ggerganov/llama.cpp/tree/master/examples/finetune",children:"more about FineTune here."}),"."]}),"\n",(0,o.jsx)(n.h1,{id:"export-lora-as-a-model",children:"Export LoRA as a model"}),"\n",(0,o.jsxs)(n.p,{children:["Now you can perform FineTune directly in LLMFarm. To do this, go to settings -> Merge Lora\nYou can read about how to apply an adapter to a model and export the resulting model ",(0,o.jsx)(n.a,{href:"https://github.com/ggerganov/llama.cpp/tree/master/examples/export-lora",children:"here."}),".\n",(0,o.jsx)(n.em,{children:"For proper functionality on iOS devices, it is recommended to merge with Q4_K models and below."})]})]})}function p(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,o.jsx)(n,{...e,children:(0,o.jsx)(d,{...e})}):d(e)}},8453:(e,n,t)=>{t.d(n,{R:()=>s,x:()=>i});var o=t(6540);const r={},a=o.createContext(r);function s(e){const n=o.useContext(a);return o.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function i(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:s(e.components),o.createElement(a.Provider,{value:n},e.children)}}}]);